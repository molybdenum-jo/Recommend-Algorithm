{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/molybdenum-jo/Recommend-Algorithm/blob/main/model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4HzLSi8-E5P",
        "outputId": "a5b3fd50-f129-4b6f-a778-75b13933017c"
      },
      "id": "L4HzLSi8-E5P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.10/dist-packages (1.1.3)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.22.4)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from surprise import Reader, Dataset, SVD\n",
        "from surprise import KNNWithMeans\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise import accuracy\n",
        "from surprise import KNNBasic"
      ],
      "metadata": {
        "id": "gPSql6sV-Dpn"
      },
      "id": "gPSql6sV-Dpn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"data/train.csv\")\n",
        "test = pd.read_csv(\"data/test.csv\")\n",
        "sub = pd.read_csv(\"data/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "3jCmQqYv-d_b"
      },
      "id": "3jCmQqYv-d_b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"Train (3).csv\")"
      ],
      "metadata": {
        "id": "EC00hHbpQB38"
      },
      "id": "EC00hHbpQB38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e92a73ee",
      "metadata": {
        "id": "e92a73ee"
      },
      "source": [
        "# Ïª®ÌÖêÏ∏† Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43049ad5",
      "metadata": {
        "id": "43049ad5"
      },
      "source": [
        "üôã‚Äç‚ôÇÔ∏è Í∞ÄÏÑ§ 2: **ÏΩòÌÖêÏ∏† Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ Í∏∞Î∞òÏùò Ï∂îÏ≤ú ÏãúÏä§ÌÖú**Ïù¥ ÎèÑÏÑú ÌèâÏ†ê ÏòàÏ∏°Ïóê Ìö®Í≥ºÏ†ÅÏùº Í≤ÉÏù¥Îã§.\n",
        "\n",
        "- Í≤∞Í≥ºÎ¨º: Ï±ÖÏùò ÏÜçÏÑ±(Ï†ÄÏûê, Ï∂úÌåêÏÇ¨, Ïû•Î•¥ Îì±)ÏùÑ Í∏∞Î∞òÏúºÎ°ú Ìïú **ÏΩòÌÖêÏ∏† Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅ Î™®Îç∏**ÏùÑ Íµ¨ÌòÑÌïòÍ≥†, Î™®Îç∏Ïùò **ÌèâÏ†ê ÏòàÏ∏° ÏÑ±Îä•**ÏùÑ ÌèâÍ∞ÄÌïúÎã§. ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ Î™®Îç∏Í≥º ÏÑ±Îä•ÏùÑ ÎπÑÍµêÌïòÏó¨ ÏΩòÌÖêÏ∏† Í∏∞Î∞ò ÌïÑÌÑ∞ÎßÅÏùò Ìö®Í≥ºÎ•º Î∂ÑÏÑùÌïúÎã§."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac75ad9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6ac75ad9",
        "outputId": "680296d6-02bf-4cd2-c603-0857682f02a1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "7739              ARKANSAS TRAVELER (BENNI HARPER MYSTERY)\n",
              "24786                  GULLIVER'S TRAVELS (SIGNET CLASSIC)\n",
              "13561              THE TIME TRAVELER'S WIFE (HARVEST BOOK)\n",
              "5107     THE TIME TRAVELER'S WIFE (TODAY SHOW BOOK CLUB...\n",
              "27488    ROAD LESS TRAVELED : A NEW PSYCHOLOGY OF LOVE,...\n",
              "20890    HOW TO TRAVEL WITH A SALMON &AMP; OTHER ESSAYS...\n",
              "24023    THE UNSAVVY TRAVELER: WOMEN'S COMIC TALES OF C...\n",
              "16088    THE MINDFUL TRAVELER: A GUIDE TO JOURNALING AN...\n",
              "26653    GUTSY MAMAS: TRAVEL TIPS AND WISDOM FOR MOTHER...\n",
              "2225                     SISTERHOOD OF THE TRAVELING PANTS\n",
              "26073    TOURIST GAZE: LEISURE AND TRAVEL IN CONTEMPORA...\n",
              "20853    GULLIFUR'S TRAVELS (ADVENTURES OF WISHBONE, NO...\n",
              "2254     THE ROAD LESS TRAVELED, 25TH ANNIVERSARY EDITI...\n",
              "6646         BAEDEKER AUSTRALIA (BAEDEKER'S TRAVEL GUIDES)\n",
              "2299            TRAVELS WITH CHARLEY: IN SEARCH OF AMERICA\n",
              "7256                                     THE ART OF TRAVEL\n",
              "17195    SONGS FOR THE OPEN ROAD : POEMS OF TRAVEL AND ...\n",
              "28154    WALK HISTORIC HALIFAX: WALKING GUIDE TO AN HIS...\n",
              "8318                 GULLIVERS TRAVELS (KONEMANN CLASSICS)\n",
              "8304                     IF ON A WINTER'S NIGHT A TRAVELER\n",
              "24033    TALES FROM A TRAVELING COUCH: A PSYCHOTHERAPIS...\n",
              "4420                                        TRAVELING SOLO\n",
              "20625    DESTINATION FREEDOM A TIME TRAVEL ADVENTURE ST...\n",
              "8289     A JOURNEY OF ONE'S OWN: UNCOMMON ADVICE FOR TH...\n",
              "15180            NEITHER HERE NOR THERE: TRAVELS IN EUROPE\n",
              "2732                              THE TIME TRAVELER'S WIFE\n",
              "23126    TRAVEL SMARTS: GETTING THE MOST FOR YOUR TRAVE...\n",
              "13415           TRAVELS WITH CHARLEY: IN SEARCH OF AMERICA\n",
              "7687                              THE TIME TRAVELER'S WIFE\n",
              "21464    SONGS FOR THE OPEN ROAD : POEMS OF TRAVEL AND ...\n",
              "1320                     IF ON A WINTER'S NIGHT A TRAVELER\n",
              "9178                 THE SISTERHOOD OF THE TRAVELING PANTS\n",
              "17554              CONSUMER REPORTS BEST TRAVEL DEALS 2000\n",
              "15249    TRAVELS IN A THIN COUNTRY: A JOURNEY THROUGH C...\n",
              "5069      TRAVELLING KIND (IDAHO) (JANET DAILEY AMERICANA)\n",
              "19725    TRAVELERS' MEDICAL ALERT SERIES: MEXICO A GUID...\n",
              "24838    JOURNEYS THROUGH TIME: A YOUNG TRAVELER'S GUID...\n",
              "24133                    SISTERHOOD OF THE TRAVELING PANTS\n",
              "10494       DANZIGER'S TRAVELS: BEYOND FORBIDDEN FRONTIERS\n",
              "3126                   THE BACKPACKER (SUMMERSDALE TRAVEL)\n",
              "16697                    IF ON A WINTER'S NIGHT A TRAVELER\n",
              "18790    STRANGER IN THE FOREST: ON FOOT ACROSS BORNEO ...\n",
              "26733    GUTSY WOMEN: TRAVEL TIPS AND WISDOM FOR THE RO...\n",
              "23452    TRAVELS WITH MY AUNT (TWENTIETH CENTURY CLASSICS)\n",
              "9114                                  TRAVELS WITH CHARLEY\n",
              "20613    SURFERS OF THE ZUVUYA: TALES OF INTERDIMENSION...\n",
              "347      THE LOST CONTINENT: TRAVELS IN SMALL-TOWN AMERICA\n",
              "1257             NEITHER HERE NOR THERE: TRAVELS IN EUROPE\n",
              "8027              TOOTH FAIRY TRAVELS (FAIRY SCHOOL, NO 1)\n",
              "26207            TRAVELING MERCIES: SOME THOUGHTS ON FAITH\n",
              "909      THE LOST CONTINENT: TRAVELS IN SMALL TOWN AMERICA\n",
              "15704    ADVENTURING IN NEW ZEALAND: THE SIERRA CLUB TR...\n",
              "16529                    SISTERHOOD OF THE TRAVELING PANTS\n",
              "8128     THE ALL-TRUE TRAVELS AND ADVENTURES OF LIDIE N...\n",
              "23945                THE SISTERHOOD OF THE TRAVELING PANTS\n",
              "17085                    IF ON A WINTER'S NIGHT A TRAVELER\n",
              "Name: Book-Title, dtype: object"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Book-Title\n",
              "9178               THE SISTERHOOD OF THE TRAVELING PANTS\n",
              "4420                                      TRAVELING SOLO\n",
              "17588                                         SISTERHOOD\n",
              "24377                                      PANTS ON FIRE\n",
              "24033  TALES FROM A TRAVELING COUCH: A PSYCHOTHERAPIS...\n",
              "2522   DANCING IN MY NUDDY- PANTS (CONFESSIONS OF GEO...\n",
              "26207          TRAVELING MERCIES: SOME THOUGHTS ON FAITH\n",
              "27937  THE DIVINE SECRETS OF THE YA-YA SISTERHOOD: A ...\n",
              "3160                               THE RAPTURE OF CANAAN\n",
              "12691    DIVINE SECRETS OF THE YA-YA SISTERHOOD: A NOVEL"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ec3bb25-e90e-405e-b169-e97c05a5f30d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Book-Title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9178</th>\n",
              "      <td>THE SISTERHOOD OF THE TRAVELING PANTS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4420</th>\n",
              "      <td>TRAVELING SOLO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17588</th>\n",
              "      <td>SISTERHOOD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24377</th>\n",
              "      <td>PANTS ON FIRE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24033</th>\n",
              "      <td>TALES FROM A TRAVELING COUCH: A PSYCHOTHERAPIS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2522</th>\n",
              "      <td>DANCING IN MY NUDDY- PANTS (CONFESSIONS OF GEO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26207</th>\n",
              "      <td>TRAVELING MERCIES: SOME THOUGHTS ON FAITH</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27937</th>\n",
              "      <td>THE DIVINE SECRETS OF THE YA-YA SISTERHOOD: A ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3160</th>\n",
              "      <td>THE RAPTURE OF CANAAN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12691</th>\n",
              "      <td>DIVINE SECRETS OF THE YA-YA SISTERHOOD: A NOVEL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ec3bb25-e90e-405e-b169-e97c05a5f30d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3ec3bb25-e90e-405e-b169-e97c05a5f30d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3ec3bb25-e90e-405e-b169-e97c05a5f30d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df = train.sample(20000, random_state=42).copy()\n",
        "df['Book-Title'] = df['Book-Title'].str.upper()\n",
        "\n",
        "#TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidfvect = TfidfVectorizer(max_features=2000, lowercase=False)\n",
        "tfidfvect\n",
        "\n",
        "# fit_transform ÏùÑ ÌÜµÌï¥ Î≥ÄÌôò\n",
        "tfidf_overview = tfidfvect.fit_transform(df[\"Book-Title\"])\n",
        "tfidf_overview\n",
        "\n",
        "df_tfidf = pd.DataFrame(tfidf_overview.toarray(), columns=tfidfvect.get_feature_names_out())\n",
        "df_tfidf\n",
        "\n",
        "# Cosine Ïú†ÏÇ¨ÎèÑ \n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_matrix = cosine_similarity(tfidf_overview, tfidf_overview)\n",
        "cosine_matrix\n",
        "\n",
        "df_cosine = pd.DataFrame(cosine_matrix, columns=df.index, index=df.index)\n",
        "df_cosine.head()\n",
        "\n",
        "#Travel Îã®Ïñ¥ÏôÄ Ïó∞Í¥ÄÏÑ±Ïù¥ ÏûàÎäî Ï±Ö Ï∞æÏïÑÎ≥¥Í∏∞\n",
        "book = \"TRAVEL\"\n",
        "display(df.loc[df[\"Book-Title\"].str.contains(book), \"Book-Title\"])\n",
        "book_id = df.loc[df[\"Book-Title\"].str.contains(book), \"Book-Title\"].index[-1]\n",
        "book_id\n",
        "\n",
        "#ÏúÑ ÎÇ¥Ïö©ÏùÑ Ìï®ÏàòÎ°ú Ï†ïÎ¶¨ \n",
        "def find_item(book, df_cosine, df):\n",
        "    try:\n",
        "        df_id = df[df[\"Book-Title\"].str.contains(book.upper())].index[0]\n",
        "        df_item = df_cosine[df_id].drop_duplicates()\n",
        "        recomm_idx = df_item[df_item < 1].nlargest(10).index\n",
        "        recomm_item = df.loc[recomm_idx, [\"Book-Title\"]]\n",
        "        return recomm_item\n",
        "    except:\n",
        "        return \"Ï∂îÏ≤ú ÎèÑÏÑú ÏóÜÏùå\"\n",
        "\n",
        "book = \"SISTERHOOD OF THE TRAVELING PANTS\"\n",
        "find_item(book, df_cosine, df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19d72bf9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "19d72bf9",
        "outputId": "c34e4382-2f1d-428a-cd42-2920311fd644"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-80d7091c250b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Book-AuthorÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌäπÏÑ± Ï∂îÏ∂ú\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mauthor_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mauthor_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauthor_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Book-Author'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2131\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         )\n\u001b[0;32m-> 2133\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1386\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    240\u001b[0m                 \u001b[0;34m\"np.nan is an invalid document, expected byte or unicode string.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Book-TitleÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌäπÏÑ± Ï∂îÏ∂ú\n",
        "title_vectorizer = TfidfVectorizer()\n",
        "title_matrix = title_vectorizer.fit_transform(train['Book-Title'])\n",
        "\n",
        "# Book-AuthorÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌäπÏÑ± Ï∂îÏ∂ú\n",
        "author_vectorizer = TfidfVectorizer()\n",
        "author_matrix = author_vectorizer.fit_transform(train['Book-Author'])\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Year-Of-Publication Ïä§ÏºÄÏùºÎßÅ\n",
        "year_scaler = StandardScaler()\n",
        "year_scaled = year_scaler.fit_transform(train[['Year-Of-Publication']])\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "n_components = 200  # Î≥ÄÍ≤Ω Í∞ÄÎä•Ìïú Í∞íÏúºÎ°ú, ÏõêÌïòÎäî Ï∞®Ïõê ÏàòÎ•º ÏÑ†ÌÉùÌï©ÎãàÎã§.\n",
        "svd = TruncatedSVD(n_components=n_components)\n",
        "\n",
        "title_matrix_reduced = svd.fit_transform(title_matrix)\n",
        "author_matrix_reduced = svd.fit_transform(author_matrix)\n",
        "\n",
        "X = np.concatenate([title_matrix_reduced, author_matrix_reduced, year_scaled], axis=1)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞Î•º ÌïôÏäµ Î∞è ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏Î°ú Î∂ÑÌï†\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, train['Book-Rating'], test_size=0.2, random_state=42)\n",
        "\n",
        "# ÌöåÍ∑Ä Î™®Îç∏ ÌïôÏäµ\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ÏòàÏ∏° Î∞è ÏÑ±Îä• ÌèâÍ∞Ä\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "# >>> 3.819596362147247"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_path='data'\n",
        "df_submit = pd.read_csv(f\"{base_path}/sample_submission.csv\")\n",
        "df_submit.head()"
      ],
      "metadata": {
        "id": "e6Ezn1wcNQIX"
      },
      "id": "e6Ezn1wcNQIX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = y_pred[:len(df_submit)]\n",
        "y_pred"
      ],
      "metadata": {
        "id": "wT9oqyqzNYk0"
      },
      "id": "wT9oqyqzNYk0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_submit = pd.DataFrame(index=test.index)\n",
        "df_submit[\"Book-Rating\"] = abs(y_pred)\n"
      ],
      "metadata": {
        "id": "m0QkNpGRNeUs"
      },
      "id": "m0QkNpGRNeUs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "dfa63c70",
      "metadata": {
        "id": "dfa63c70"
      },
      "source": [
        "# ÏµúÍ∑ºÏ†ë Ïù¥ÏõÉ ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ (KNNBasic)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "654095fd",
      "metadata": {
        "id": "654095fd"
      },
      "source": [
        "\n",
        "üôã‚Äç‚ôÇÔ∏è Í∞ÄÏÑ§ 1: **ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ Í∏∞Î∞òÏùò Ï∂îÏ≤ú ÏãúÏä§ÌÖú**Ïù¥ ÎèÑÏÑú ÌèâÏ†ê ÏòàÏ∏°Ïóê Ìö®Í≥ºÏ†ÅÏùº Í≤ÉÏù¥Îã§.\n",
        "\n",
        "- Í≤∞Í≥ºÎ¨º: **ÏÇ¨Ïö©Ïûê Í∏∞Î∞ò ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ** Î∞è **ÏïÑÏù¥ÌÖú Í∏∞Î∞ò ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ** Î™®Îç∏ÏùÑ Íµ¨ÌòÑÌïòÍ≥†, Ïù¥Îì§ Î™®Îç∏Ïùò **ÌèâÏ†ê ÏòàÏ∏° ÏÑ±Îä•**ÏùÑ ÌèâÍ∞ÄÌïúÎã§. **RMSE Í∞í**ÏúºÎ°ú ÏÑ±Îä•ÏùÑ ÎπÑÍµêÌïòÏó¨ Ïñ¥Îñ§ ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ Î∞©Î≤ïÏù¥ Îçî ÎÇòÏùÄ ÏÑ±Îä•ÏùÑ Î≥¥Ïù¥ÎäîÏßÄ Í≤∞Ï†ïÌïúÎã§."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c3c2104",
      "metadata": {
        "id": "7c3c2104"
      },
      "source": [
        "### ÏÇ¨Ïö©Ïûê Í∏∞Î∞ò ÏµúÍ∑ºÏ†ë Ïù¥ÏõÉ ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82eb3be0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "id": "82eb3be0",
        "outputId": "8a51a535-5193-4db2-83b6-01f5767f2966"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ìï¥Îãπ Ïú†Ï†Ä :  USER_00004\n",
            "Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ïú†Ï†Ä :  USER_04985\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    Book-Title\n",
              "0                                                             \n",
              "BOOK_000212                           Don't Speak to Strangers\n",
              "BOOK_000218                            Never Nosh a Matzo Ball\n",
              "BOOK_000230              The Sisterhood of the Traveling Pants\n",
              "BOOK_000252  The Marine &amp; The Debutante  (Bachelor Batt...\n",
              "BOOK_000223                       Jane Eyre (Penguin Classics)"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8fbf45a-6ba6-487a-bbe5-0cf471ebc36d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Book-Title</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>BOOK_000212</th>\n",
              "      <td>Don't Speak to Strangers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BOOK_000218</th>\n",
              "      <td>Never Nosh a Matzo Ball</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BOOK_000230</th>\n",
              "      <td>The Sisterhood of the Traveling Pants</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BOOK_000252</th>\n",
              "      <td>The Marine &amp;amp; The Debutante  (Bachelor Batt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BOOK_000223</th>\n",
              "      <td>Jane Eyre (Penguin Classics)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8fbf45a-6ba6-487a-bbe5-0cf471ebc36d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8fbf45a-6ba6-487a-bbe5-0cf471ebc36d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8fbf45a-6ba6-487a-bbe5-0cf471ebc36d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "cs_matrix = pd.crosstab(df[\"User-ID\"], df[\"Book-ID\"])\n",
        "cs_matrix.head()\n",
        "\n",
        "# ÏÇ¨Ïö©Ïûê Í∏∞Î∞ò ÏµúÍ∑ºÏ†ë Ïù¥ÏõÉÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ \n",
        "# ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑ Íµ¨ÌïòÍ∏∞ \n",
        "\n",
        "# sklearn.metrics.pairwise ÏóêÏÑú cosine_similarity Î∂àÎü¨Ïò§Í∏∞\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Ïú†Ï†ÄÍ∞Ñ Ïú†ÏÇ¨ÎèÑ Íµ¨ÌïòÍ∏∞\n",
        "user_sim = cosine_similarity(cs_matrix, cs_matrix)\n",
        "user_sim\n",
        "\n",
        "\n",
        "# df_user_sim \n",
        "df_user_sim = pd.DataFrame(user_sim, index=cs_matrix.index, columns=cs_matrix.index)\n",
        "df_user_sim.head(5)\n",
        "\n",
        "idx = 2\n",
        "usr_sim = df_user_sim.iloc[idx]\n",
        "curr_usr = usr_sim.index[idx]\n",
        "print(\"Ìï¥Îãπ Ïú†Ï†Ä : \", curr_usr)\n",
        "most_sim_user = usr_sim[usr_sim.index != usr_sim.index[idx]].nlargest(1)\n",
        "print(\"Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ïú†Ï†Ä : \", most_sim_user.index[0])\n",
        "\n",
        "\n",
        "# ÌòÑÏû¨ Ïú†Ï†ÄÍ∞Ä Ïù¥Ïö©Ìïú Ï±Ö\n",
        "curr_user_book = cs_matrix.loc[curr_usr].nlargest(20).index\n",
        "\n",
        "# Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ïú†Ï†ÄÍ∞Ä Ïù¥Ìïú Ï±Ö\n",
        "most_usr_book = cs_matrix.loc[most_sim_user.index[0]].nlargest(20).index\n",
        "\n",
        "# Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ïú†Ï†ÄÍ∞Ä Íµ¨Îß§Ìïú Ï±Ö - ÎÇ¥Í∞Ä Íµ¨Îß§Ìïú Ï±Ö\n",
        "# ==> Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ïú†Ï†ÄÍ∞Ä Íµ¨Îß§Ìïú Ï±Ö Ï§ëÏóê ÎÇ¥Í∞Ä Íµ¨Îß§ÌïòÏßÄ ÏïäÏùÄ Ï±Ö\n",
        "recomm_book = list(set(most_usr_book) - set(curr_user_book))\n",
        "\n",
        "book_desc = train.drop_duplicates(\"Book-ID\")[[\"Book-ID\", \"Book-Title\"]]\n",
        "book_desc = book_desc.set_index(\"Book-ID\")\n",
        "book_desc.shape\n",
        "\n",
        "# df_sim_buy joinÏúºÎ°ú ÏÉÅÌíàÏùò Description Íµ¨ÌïòÍ∏∞\n",
        "pd.DataFrame(recomm_book).set_index(0).join(book_desc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4219b3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "d4219b3c",
        "outputId": "078b2611-d946-446d-c5ba-4fca9ef9874c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÌòÑÏû¨ Ïú†Ï†Ä :  USER_00329 Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ïú†Ï†Ä :  USER_04643\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ï∂îÏ≤úÌï† ÎèÑÏÑú ÏóÜÏùå'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def usr_recomm(idx, df_user_sim, cs_matrix):\n",
        "    try:\n",
        "        curr_usr = df_user_sim.index[idx]\n",
        "        usr_sim = df_user_sim.iloc[idx]\n",
        "        most_sim_user = usr_sim[usr_sim.index != usr_sim.index[idx]].nlargest(1)\n",
        "        print(\"ÌòÑÏû¨ Ïú†Ï†Ä : \", curr_usr, \"Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ïú†Ï†Ä : \", most_sim_user.index[0])\n",
        "        # ÌòÑÏû¨ Ïú†Ï†ÄÍ∞Ä Íµ¨Îß§Ìïú Ï±Ö\n",
        "        curr_user_book = cs_matrix.loc[curr_usr].nlargest(20).index\n",
        "        # Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ïú†Ï†ÄÍ∞Ä Íµ¨Îß§Ìïú Ï±Ö\n",
        "        most_usr_book = cs_matrix.loc[most_sim_user.index[0]].nlargest(20).index\n",
        "        # Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ïú†Ï†ÄÍ∞Ä Íµ¨Îß§Ìïú Ï±Ö - ÎÇ¥Í∞Ä Íµ¨Îß§Ìïú Ï±Ö\n",
        "        # ==> Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Ïú†Ï†ÄÍ∞Ä Íµ¨Îß§Ìïú Ï±Ö Ï§ëÏóê ÎÇ¥Í∞Ä Íµ¨Îß§ÌïòÏßÄ ÏïäÏùÄ Ï±Ö\n",
        "        recomm_book = list(set(most_usr_book) - set(curr_user_book))\n",
        "        reomm_result = pd.DataFrame(recomm_book).set_index(0).join(book_desc)\n",
        "        return reomm_result\n",
        "    except:\n",
        "        return \"Ï∂îÏ≤úÌï† ÎèÑÏÑú ÏóÜÏùå\"\n",
        "\n",
        "usr_idx = 150\n",
        "usr_recomm(usr_idx, df_user_sim, cs_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3456b501",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "3456b501",
        "outputId": "a471c9a1-19d9-40e9-ab04-ce69ff17eb11"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-73f24e3c52e7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNNBasic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpredictions_knn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "from surprise.prediction_algorithms.knns import KNNBasic\n",
        "\n",
        "# model\n",
        "knn = KNNBasic(name=\"cosine\", user_base=True)\n",
        "knn.fit(X_train)\n",
        "predictions_knn = knn.test(X_test)\n",
        "X_test[3]\n",
        "predictions_knn[3]\n",
        "df[\"Book-Rating\"].describe()\n",
        "accuracy.rmse(predictions_knn)\n",
        "\n",
        "\n",
        "df.sample()\n",
        "iid = \"BOOK_011665\"\n",
        "df[df[\"Book-ID\"] == iid]\n",
        "uid = 'USER_44609'\n",
        "iid = \"BOOK_011665\"\n",
        "df[(df[\"User-ID\"] == uid) & (df[\"Book-ID\"] == iid)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0986f64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0986f64",
        "outputId": "e9ef53fe-fddc-459e-b899-0e67418a2342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 3.8908\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.8907782808322637"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# ÏÉòÌîå Ï∂îÏ∂ú\n",
        "df = train[['User-ID','Book-Title', 'Book-Rating']].drop_duplicates().sample(100000, random_state=42)\n",
        "\n",
        "r_min = df['Book-Rating'].min()\n",
        "r_max = df['Book-Rating'].max()\n",
        "\n",
        "reader = Reader(rating_scale=(r_min, r_max))\n",
        "\n",
        "data = Dataset.load_from_df(\n",
        "    df[['User-ID','Book-Title', 'Book-Rating']], \n",
        "    reader)\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ÏÖã ÎÇòÎàÑÍ∏∞ \n",
        "from surprise.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train.n_users, X_train.n_items, X_train.n_ratings\n",
        "type(X_test), len(X_test)\n",
        "\n",
        "# model\n",
        "from surprise.prediction_algorithms.knns import KNNBasic\n",
        "knn = KNNBasic(name=\"cosine\",\n",
        "               user_base=True,\n",
        "               k = 20,\n",
        "               min_k = 15,\n",
        "               min_support = 3,\n",
        "               verbose = False\n",
        "              )\n",
        "# ÌõàÎ†® Î∞è ÌÖåÏä§Ìä∏\n",
        "knn.fit(X_train)\n",
        "predictions = knn.test(X_test)\n",
        "\n",
        "# rmse Í∞í\n",
        "accuracy.rmse(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b929984",
      "metadata": {
        "id": "7b929984"
      },
      "source": [
        "### ÏïÑÏù¥ÌÖú Í∏∞Î∞ò ÏµúÍ∑ºÏ†ë Ïù¥ÏõÉ ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d08d92",
      "metadata": {
        "id": "b3d08d92"
      },
      "outputs": [],
      "source": [
        "knni = KNNBasic(name=\"cosine\", user_base=False)\n",
        "knni.fit(X_train)\n",
        "predictions_item = knni.test(validset)\n",
        "accuracy.rmse(predictions_item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a776f99d",
      "metadata": {
        "id": "a776f99d"
      },
      "outputs": [],
      "source": [
        "# ÏÉòÌîå Ï∂îÏ∂ú\n",
        "df = train[['User-ID','Book-Title', 'Book-Rating']].drop_duplicates().sample(200000, random_state=42)\n",
        "\n",
        "r_min = df['Book-Rating'].min()\n",
        "r_max = df['Book-Rating'].max()\n",
        "\n",
        "reader = Reader(rating_scale=(r_min, r_max))\n",
        "\n",
        "data = Dataset.load_from_df(\n",
        "    df[['User-ID','Book-Title', 'Book-Rating']], \n",
        "    reader)\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ÏÖã ÎÇòÎàÑÍ∏∞ \n",
        "from surprise.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train.n_users, X_train.n_items, X_train.n_ratings\n",
        "type(X_test), len(X_test)\n",
        "\n",
        "from surprise.prediction_algorithms.knns import KNNBasic\n",
        "# model\n",
        "\n",
        "# KNNBasic model\n",
        "knni = KNNBasic(name=\"cosine\", user_base=False)\n",
        "\n",
        "knn.fit(X_train)\n",
        "predictions = knn.test(X_test)\n",
        "\n",
        "# accuracy.rmse\n",
        "accuracy.rmse(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51b3897",
      "metadata": {
        "id": "c51b3897"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9db19286",
      "metadata": {
        "id": "9db19286"
      },
      "source": [
        "# Ïû†Ïû¨ ÏöîÏù∏ ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ (SVD)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d1e4dd8",
      "metadata": {
        "id": "1d1e4dd8"
      },
      "source": [
        "üôã‚Äç‚ôÇÔ∏è Í∞ÄÏÑ§ 3: **ÌñâÎ†¨ Ïù∏ÏàòÎ∂ÑÌï¥ Í∏∞Î∞òÏùò Ï∂îÏ≤ú ÏãúÏä§ÌÖú**Ïù¥ ÎèÑÏÑú ÌèâÏ†ê ÏòàÏ∏°Ïóê Ìö®Í≥ºÏ†ÅÏùº Í≤ÉÏù¥Îã§.\n",
        "\n",
        "- Í≤∞Í≥ºÎ¨º: **SVD**ÏôÄ **ALS**ÏôÄ Í∞ôÏùÄ **ÌñâÎ†¨ Ïù∏ÏàòÎ∂ÑÌï¥ Í∏∞Î≤ï**ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ÏùÑ Íµ¨ÌòÑÌïòÍ≥†, Î™®Îç∏Ïùò **ÌèâÏ†ê ÏòàÏ∏° ÏÑ±Îä•**ÏùÑ ÌèâÍ∞ÄÌïúÎã§. Îã§Î•∏ Ï∂îÏ≤ú ÏãúÏä§ÌÖúÍ≥º ÏÑ±Îä•ÏùÑ ÎπÑÍµêÌïòÏó¨ ÌñâÎ†¨ Ïù∏ÏàòÎ∂ÑÌï¥Ïùò Ìö®Í≥ºÎ•º Î∂ÑÏÑùÌïúÎã§.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a87bcfd0",
      "metadata": {
        "id": "a87bcfd0"
      },
      "outputs": [],
      "source": [
        "svd = SVD(random_state=42)\n",
        "svd.fit(X_train)\n",
        "X_test = [(f\"TEST_{uid[6:]}\", iid, rating) for uid, iid, rating in X_test]\n",
        "predictions_svd = svd.test(X_test)\n",
        "\n",
        "pred_svd = [pred.est for pred in predictions_svd]\n",
        "pred_svd = np.array(pred_svd)\n",
        "pred_svd.shape\n",
        "\n",
        "df_submit = pd.read_csv(f\"{base_path}/sample_submission.csv\")\n",
        "df_submit.head()\n",
        "\n",
        "pred_svd = pred_svd[:len(df_submit)]\n",
        "pred_svd\n",
        "\n",
        "df_submit[\"Book-Rating\"] = abs(pred_svd)\n",
        "\n",
        "file_name = f\"{base_path}/submit.csv\"\n",
        "df_submit.to_csv(file_name, index=False)\n",
        "pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e35f2e",
      "metadata": {
        "id": "46e35f2e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "sub = pd.read_csv(\"sample_submission.csv\")\n",
        "train['Age'] = np.where((train['Age'] == 0) | (train['Age'] >= 80), 35, train['Age'])\n",
        "test['Age'] = np.where((test['Age'] == 0) | (test['Age'] >= 80), 35, test['Age'])\n",
        "train['Year-Of-Publication'] = np.where(train['Year-Of-Publication'] == -1, 1997, train['Year-Of-Publication'])\n",
        "test['Year-Of-Publication'] = np.where(test['Year-Of-Publication'] == -1, 1997, test['Year-Of-Publication'])\n",
        "import pandas as pd\n",
        "from surprise import Dataset, Reader\n",
        "from surprise import SVD\n",
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "user_item_rating = train[[\"User-ID\", \"Book-ID\", \"Book-Rating\"]]\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞Î•º Surprise ÎùºÏù¥Î∏åÎü¨Î¶¨ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
        "reader = Reader(rating_scale=(1, 10))\n",
        "data = Dataset.load_from_df(user_item_rating, reader)\n",
        "\n",
        "# Train/Test Î∂ÑÎ¶¨\n",
        "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
        "# SVD ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ Íµ¨Ï∂ï Î∞è ÌïôÏäµ\n",
        "algo = SVD()\n",
        "algo.fit(trainset)\n",
        "\n",
        "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú ÏòàÏ∏°\n",
        "predictions = algo.test(testset)\n",
        "# ÏÑ±Îä• ÌèâÍ∞Ä (RMSE)\n",
        "from surprise import accuracy\n",
        "rmse = accuracy.rmse(predictions)\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        ">>> 3.4935\n",
        "\n",
        "# ÏõêÎûòÏùò ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Book-Rating ÏòàÏ∏°\n",
        "test[\"Book-Rating\"] = test.apply(lambda row: algo.predict(row[\"User-ID\"], row[\"Book-ID\"]).est, axis=1)\n",
        "\n",
        "# Í≤∞Í≥ºÎ•º Ï†ÄÏû•Ìï† Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ± Î∞è Í≤∞Ìï©\n",
        "sub = sub.copy()\n",
        "sub[\"Book-Rating\"] = test[\"Book-Rating\"]\n",
        "\n",
        "# Í≤∞Í≥ºÎ•º CSV ÌååÏùºÎ°ú Ï†ÄÏû•\n",
        "sub.to_csv(f\"svd{rmse:.4f}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86623b14",
      "metadata": {
        "id": "86623b14"
      },
      "outputs": [],
      "source": [
        "df = train[['User-ID','ÏúÑÏπò+Ï±Öid', 'Book-Rating']]\n",
        "\n",
        "# reader \n",
        "r_min = df['Book-Rating'].min()\n",
        "r_max = df['Book-Rating'].max()\n",
        "reader = Reader(rating_scale=(r_min, r_max))\n",
        "\n",
        "\n",
        "# Dataset.load_from_df Î°ú Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌï©ÎãàÎã§.\n",
        "data = Dataset.load_from_df(\n",
        "    df[['User-ID','ÏúÑÏπò+Ï±Öid', 'Book-Rating']], \n",
        "    reader)\n",
        "\n",
        "\n",
        "# train, test Îç∞Ïù¥ÌÑ∞ÏÖã ÎÇòÎàÑÍ∏∞ \n",
        "from surprise.model_selection import train_test_split\n",
        "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "svd = SVD()\n",
        "\n",
        "# fit, test\n",
        "svd.fit(X_train)\n",
        "predictions_svd = svd.test(X_test)\n",
        "\n",
        "\n",
        "# rmse Ï†êÏàò ÌôïÏù∏ÌïòÍ∏∞ \n",
        "accuracy.rmse(predictions_svd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb3ad1a6",
      "metadata": {
        "id": "eb3ad1a6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train/Validation Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
        "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†ê ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_mean_rating = train_data.groupby('User-ID')['Book-Rating'].mean().reset_index()\n",
        "user_mean_rating.columns = ['User-ID', 'User-Mean-Rating']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏù¥ Í≤∞Ï∏°ÏπòÏù∏ Í≤ΩÏö∞, Ï†ÑÏ≤¥ ÌèâÍ∑† ÌèâÏ†êÏúºÎ°ú ÎåÄÏ≤¥\n",
        "mean_rating = train_data['Book-Rating'].mean()\n",
        "train_data['User-Mean-Rating'] = train_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "val_data['User-Mean-Rating'] = val_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ Ìè¨Ìï®Ìïú ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±\n",
        "user_item_rating_mean = train_data[[\"User-ID\", \"Book-ID\", \"User-Mean-Rating\"]]\n",
        "\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞Î•º Surprise ÎùºÏù¥Î∏åÎü¨Î¶¨ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
        "reader = Reader(rating_scale=(1, 10))\n",
        "data_mean = Dataset.load_from_df(user_item_rating_mean, reader)\n",
        "\n",
        "# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î•º trainsetÏúºÎ°ú Î≥ÄÌôò\n",
        "trainset = data_mean.build_full_trainset()\n",
        "\n",
        "# SVD ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ Íµ¨Ï∂ï Î∞è ÌïôÏäµ\n",
        "algo = SVD()\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú ÏòàÏ∏°\n",
        "val_data[\"predicted_rating\"] = val_data.apply(lambda row: algo.predict(row[\"User-ID\"], row[\"Book-ID\"]).est, axis=1)\n",
        "\n",
        "# ÏÑ±Îä• ÌèâÍ∞Ä (RMSE)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "rmse = mean_squared_error(val_data[\"Book-Rating\"], val_data[\"predicted_rating\"], squared=False)\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a905d58",
      "metadata": {
        "id": "2a905d58"
      },
      "source": [
        "### SVD ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ Ï°∞Ï†ï"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2bfd30",
      "metadata": {
        "id": "fd2bfd30"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "sub = pd.read_csv(\"sample_submission.csv\")\n",
        "train['Age'] = np.where((train['Age'] == 0) | (train['Age'] >= 80), 35, train['Age'])\n",
        "test['Age'] = np.where((test['Age'] == 0) | (test['Age'] >= 80), 35, test['Age'])\n",
        "train['Year-Of-Publication'] = np.where(train['Year-Of-Publication'] == -1, 1997, train['Year-Of-Publication'])\n",
        "test['Year-Of-Publication'] = np.where(test['Year-Of-Publication'] == -1, 1997, test['Year-Of-Publication'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train/Validation Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
        "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†ê ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_mean_rating = train_data.groupby('User-ID')['Book-Rating'].mean().reset_index()\n",
        "user_mean_rating.columns = ['User-ID', 'User-Mean-Rating']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏù¥ Í≤∞Ï∏°ÏπòÏù∏ Í≤ΩÏö∞, Ï†ÑÏ≤¥ ÌèâÍ∑† ÌèâÏ†êÏúºÎ°ú ÎåÄÏ≤¥\n",
        "mean_rating = train_data['Book-Rating'].mean()\n",
        "train_data['User-Mean-Rating'] = train_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "val_data['User-Mean-Rating'] = val_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ Ìè¨Ìï®Ìïú ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±\n",
        "user_item_rating_mean = train_data[[\"User-ID\", \"Book-ID\", \"User-Mean-Rating\"]]\n",
        "# Îç∞Ïù¥ÌÑ∞Î•º Surprise ÎùºÏù¥Î∏åÎü¨Î¶¨ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
        "reader = Reader(rating_scale=(1, 10))\n",
        "data_mean = Dataset.load_from_df(user_item_rating_mean, reader)\n",
        "# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î•º trainsetÏúºÎ°ú Î≥ÄÌôò\n",
        "trainset = data_mean.build_full_trainset()\n",
        "\n",
        "# SVD ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ Íµ¨Ï∂ï Î∞è ÌïôÏäµ\n",
        "algo = SVD(random_state=42,\n",
        "          n_epochs= 100 , \n",
        "          lr_all= 0.01,\n",
        "          n_factors = 200,\n",
        "          reg_all = 0.06,\n",
        "          verbose=True)\n",
        "algo.fit(trainset)\n",
        "\n",
        "\n",
        "# Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú ÏòàÏ∏°\n",
        "val_data[\"predicted_rating\"] = val_data.apply(lambda row: algo.predict(row[\"User-ID\"], row[\"Book-ID\"]).est, axis=1)\n",
        "# ÏÑ±Îä• ÌèâÍ∞Ä (RMSE)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "rmse = mean_squared_error(val_data[\"Book-Rating\"], val_data[\"predicted_rating\"], squared=False)\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        ">>> RMSE: 3.4074\n",
        "\n",
        "\n",
        "# ÏõêÎûòÏùò ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Book-Rating ÏòàÏ∏°\n",
        "test[\"Book-Rating\"] = test.apply(lambda row: algo.predict(row[\"User-ID\"], row[\"Book-ID\"]).est, axis=1)\n",
        "\n",
        "# Í≤∞Í≥ºÎ•º Ï†ÄÏû•Ìï† Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ± Î∞è Í≤∞Ìï©\n",
        "sub = sub.copy()\n",
        "sub[\"Book-Rating\"] = test[\"Book-Rating\"]\n",
        "\n",
        "# Í≤∞Í≥ºÎ•º CSV ÌååÏùºÎ°ú Ï†ÄÏû•\n",
        "sub.to_csv(f\"svd{rmse:.4f}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9e54803",
      "metadata": {
        "id": "c9e54803"
      },
      "source": [
        "### SVD++ ÏïåÍ≥†Î¶¨Ï¶ò"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec9bb876",
      "metadata": {
        "id": "ec9bb876"
      },
      "outputs": [],
      "source": [
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "from surprise import SVDpp\n",
        "from surprise import accuracy\n",
        "from surprise.model_selection import train_test_split\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ \n",
        "# Ïú†Ï†Ä - ÏïÑÏù¥ÌÖú - ÌèâÏ†ê Îç∞Ïù¥ÌÑ∞ÏÖã ÎßåÎì§Í∏∞ \n",
        "df = train_data[[\"User-ID\", \"Book-Title\", \"Book-Rating\"]].sample(300000)\n",
        "\n",
        "# reader \n",
        "r_min = df_tr['Book-Rating'].min()\n",
        "r_max = df_tr['Book-Rating'].max()\n",
        "reader = Reader(rating_scale=(r_min, r_max))\n",
        "\n",
        "\n",
        "# Dataset.load_from_df Î°ú Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌï©ÎãàÎã§.\n",
        "data = Dataset.load_from_df(\n",
        "    df[[\"User-ID\", \"Book-Title\", \"Book-Rating\"]], \n",
        "    reader)\n",
        "\n",
        "from surprise.model_selection import KFold\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú\n",
        "reader = Reader(rating_scale=(1, 10))\n",
        "data = Dataset.load_from_df(df[[\"User-ID\", \"Book-Title\", \"Book-Rating\"]], reader)\n",
        "\n",
        "# k-fold ÍµêÏ∞® Í≤ÄÏ¶ùÏóê ÏÇ¨Ïö©Ìï† Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
        "kf = KFold(n_splits=5)\n",
        "\n",
        "for trainset, testset in kf.split(data):\n",
        "    # SVD++ Î™®Îç∏ ÏÉùÏÑ± Î∞è ÌïôÏäµ\n",
        "    model = SVDpp(n_factors=50, lr_all=0.01, reg_all=0.1)\n",
        "    model.fit(trainset)\n",
        "\n",
        "    # ÌÖåÏä§Ìä∏ÏÖãÏúºÎ°ú ÏòàÏ∏° Î∞è ÌèâÍ∞Ä\n",
        "    3.530020675576203predictions = model.test(testset)\n",
        "    rmse = accuracy.rmse(predictions)\n",
        "    print('RMSE:', rmse)\n",
        "\n",
        ">>> 3.530020675576203"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "622cf200",
      "metadata": {
        "id": "622cf200"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Train/Validation Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
        "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†ê ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_mean_rating = train_data.groupby('User-ID')['Book-Rating'].mean().reset_index()\n",
        "user_mean_rating.columns = ['User-ID', 'User-Mean-Rating']\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏù¥ Í≤∞Ï∏°ÏπòÏù∏ Í≤ΩÏö∞, Ï†ÑÏ≤¥ ÌèâÍ∑† ÌèâÏ†êÏúºÎ°ú ÎåÄÏ≤¥\n",
        "mean_rating = train_data['Book-Rating'].mean()\n",
        "train_data['User-Mean-Rating'] = train_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "val_data['User-Mean-Rating'] = val_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàò ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_rating_count = train_data.groupby('User-ID')['Book-Rating'].count().reset_index()\n",
        "user_rating_count.columns = ['User-ID', 'User-Rating-Count']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàòÎ•º train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "\n",
        "user_item_rating_mean = train_data[[\"User-ID\", \"Book-ID\", 'User-Mean-Rating']].sample(30000)\n",
        "reader = Reader(rating_scale=(0, 10))\n",
        "data_mean = Dataset.load_from_df(user_item_rating_mean, reader)\n",
        "trainset = data_mean.build_full_trainset()\n",
        "\n",
        "\n",
        "\n",
        "# SVD++ ÏïåÍ≥†Î¶¨Ï¶òÏúºÎ°ú Î™®Îç∏ ÏÉùÏÑ± Î∞è ÌïôÏäµ\n",
        "algo = SVDpp(random_state=42)\n",
        "algo.fit(trainset)\n",
        "\n",
        "val_data[\"predicted_rating\"] = val_data.apply(lambda row: svd.predict(row[\"User-ID\"], row[\"Book-ID\"]).est, axis=1)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "rmse = mean_squared_error(val_data[\"Book-Rating\"], val_data[\"predicted_rating\"], squared=False)\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        ">>> 3.5217"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32130f4a",
      "metadata": {
        "id": "32130f4a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c22f1472",
      "metadata": {
        "id": "c22f1472"
      },
      "source": [
        "# ÏïôÏÉÅÎ∏î"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc1c243c",
      "metadata": {
        "id": "bc1c243c"
      },
      "source": [
        "üôã‚Äç‚ôÇÔ∏è Í∞ÄÏÑ§ 5: **ÏïôÏÉÅÎ∏î Í∏∞Î≤ïÏùÑ ÏÇ¨Ïö©Ìïú Ï∂îÏ≤ú ÏãúÏä§ÌÖú**Ïù¥ ÎèÑÏÑú ÌèâÏ†ê ÏòàÏ∏°Ïóê Ìö®Í≥ºÏ†ÅÏùº Í≤ÉÏù¥Îã§.\n",
        "\n",
        "- Í≤∞Í≥ºÎ¨º: Ïó¨Îü¨ Ï∂îÏ≤ú ÏãúÏä§ÌÖú Î™®Îç∏ÏùÑ Í≤∞Ìï©ÌïòÏó¨ ÏïôÏÉÅÎ∏î Î™®Îç∏ÏùÑ Íµ¨ÌòÑÌïúÎã§. **Í∞ÄÏ§ë ÌèâÍ∑†**, **Ïä§ÌÉúÌÇπ(Stacking)** Îì±Ïùò ÏïôÏÉÅÎ∏î Í∏∞Î≤ïÏùÑ Ï†ÅÏö©ÌïòÏó¨ Î™®Îç∏Ïùò ÌèâÏ†ê ÏòàÏ∏° ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïúÎã§. Îã®Ïùº Î™®Îç∏Í≥º ÏÑ±Îä•ÏùÑ ÎπÑÍµêÌïòÏó¨ ÏïôÏÉÅÎ∏î Í∏∞Î≤ïÏùò Ìö®Í≥ºÎ•º Î∂ÑÏÑùÌïúÎã§."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae786ad",
      "metadata": {
        "id": "dae786ad"
      },
      "source": [
        "### xgboost + Randomforest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa4ca9a",
      "metadata": {
        "id": "afa4ca9a"
      },
      "outputs": [],
      "source": [
        "# Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨\n",
        "categorical_feature = train.select_dtypes(exclude=\"number\").columns\n",
        "categorical_feature\n",
        "\n",
        "train[categorical_feature] = train[categorical_feature].astype(\"category\")\n",
        "test[categorical_feature] = test[categorical_feature].astype(\"category\")\n",
        "train.info(), test.info()\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train[categorical_feature] = oe.fit_transform(train[categorical_feature])\n",
        "test[categorical_feature] = oe.transform(test[categorical_feature])\n",
        "\n",
        "X = train.drop(columns=\"Book-Rating\")\n",
        "y = train[\"Book-Rating\"]\n",
        "X.shape, y.shape\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size = 0.1, random_state=42\n",
        ")\n",
        "\n",
        "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape\n",
        "X_test = test\n",
        "X_test.shape\n",
        "\n",
        "# model \n",
        "import xgboost as xgb\n",
        "\n",
        "model_xgb = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=250, \n",
        "                             max_depth=3,\n",
        "                             random_state=42, \n",
        "                             n_jobs=-1)\n",
        "                             \n",
        "model_xgb\n",
        "\n",
        "dtrain = xgb.DMatrix(X, label=y)\n",
        "from xgboost.callback import EarlyStopping\n",
        "# fit\n",
        "es = xgb.callback.EarlyStopping(\n",
        "    rounds=2,\n",
        "    save_best=True,\n",
        "    maximize=False,\n",
        "    data_name=\"validation_0\",\n",
        "    metric_name=\"rmse\",\n",
        ")\n",
        "\n",
        "model_xgb.fit(X_train, y_train, \n",
        "              eval_set=[(X_valid, y_valid)], callbacks=[es])\n",
        "\n",
        "y_valid_predict = model_xgb.predict(X_valid)\n",
        "y_valid_predict[:5]\n",
        "\n",
        "fi = pd.Series(model_xgb.feature_importances_)\n",
        "fi.index = model_xgb.feature_names_in_\n",
        "fi.nlargest(20).plot.barh()\n",
        "\n",
        "# model_xgb.plot_importance\n",
        "xgb.plot_importance(model_xgb, max_num_features=20);\n",
        "\n",
        "\n",
        "xgb.plot_tree(model_xgb, num_trees=0)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(30, 20)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(y_valid, y_valid_predict)\n",
        "\n",
        "score_xgb = model_xgb.score(X_valid, y_valid)\n",
        "score_xgb\n",
        "\n",
        "X_test.shape\n",
        "\n",
        "y_pred_xgb = model_xgb.predict(X_test)\n",
        "y_pred_xgb\n",
        "\n",
        "df_submit = pd.read_csv(f\"{base_path}/sample_submission.csv\")\n",
        "y_pred_xgb = y_pred_xgb[:len(df_submit)].round()\n",
        "df_submit[\"Book-Rating\"] = abs(y_pred_xgb)\n",
        "\n",
        "# Ï†ÄÏû• \n",
        "file_name = f\"{base_path}/submit.csv\"\n",
        "df_submit.to_csv(file_name, index=False)\n",
        "pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4138b97",
      "metadata": {
        "id": "a4138b97"
      },
      "source": [
        "### Decision Tree + xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa8b8bc8",
      "metadata": {
        "id": "aa8b8bc8"
      },
      "outputs": [],
      "source": [
        "categorical_feature = train.select_dtypes(exclude=\"number\").columns\n",
        "categorical_feature\n",
        "\n",
        "train[categorical_feature] = train[categorical_feature].astype(\"category\")\n",
        "test[categorical_feature] = test[categorical_feature].astype(\"category\")\n",
        "train.info(), test.info()\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train[categorical_feature] = oe.fit_transform(train[categorical_feature])\n",
        "test[categorical_feature] = oe.transform(test[categorical_feature])\n",
        "\n",
        "X = train.drop(columns=\"Book-Rating\")\n",
        "y = train[\"Book-Rating\"]\n",
        "X.shape, y.shape\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size = 0.1, random_state=42\n",
        ")\n",
        "\n",
        "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape\n",
        "X_test = test\n",
        "X_test.shape\n",
        "\n",
        "# model \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "tree_model = DecisionTreeClassifier()\n",
        "xgb_model = XGBClassifier()\n",
        "\n",
        "oting_model = VotingClassifier(estimators=[('tree', tree_model), ('xgb', xgb_model)], voting='hard')\n",
        "voting_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = voting_model.predict(X_test)\n",
        "rmse_tf_xg = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f\"rmse_tf_xg: {rmse_tf_xg}\")\n",
        ">>> 4.686224381869224"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6211c95e",
      "metadata": {
        "id": "6211c95e"
      },
      "source": [
        "### Desicion tree + tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba74dab7",
      "metadata": {
        "id": "ba74dab7"
      },
      "outputs": [],
      "source": [
        "categorical_feature = train.select_dtypes(exclude=\"number\").columns\n",
        "categorical_feature\n",
        "\n",
        "train[categorical_feature] = train[categorical_feature].astype(\"category\")\n",
        "test[categorical_feature] = test[categorical_feature].astype(\"category\")\n",
        "train.info(), test.info()\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train[categorical_feature] = oe.fit_transform(train[categorical_feature])\n",
        "test[categorical_feature] = oe.transform(test[categorical_feature])\n",
        "\n",
        "X = train.drop(columns=\"Book-Rating\")\n",
        "y = train[\"Book-Rating\"]\n",
        "X.shape, y.shape\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size = 0.1, random_state=42\n",
        ")\n",
        "\n",
        "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape\n",
        "X_test = test\n",
        "X_test.shape\n",
        "\n",
        "# model \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "tree_model = DecisionTreeClassifier()\n",
        "import tensorflow as tf\n",
        "tf.model = tf.keras.Sequential([\n",
        "    # tf.keras.layers.BatchNormalization(input_shape=X_train.iloc[0].shape),\n",
        "    tf.keras.layers.Dense(units=64, activation='relu', input_shape=X_train.iloc[0].shape),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=64),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(units=1)\n",
        "])\n",
        "voting_model_tt = VotingClassifier(estimators=[('tree', tree_model), ('tensorflow', xgb_model)], voting='hard')\n",
        "voting_model_tt.fit(X_train, y_train)\n",
        "\n",
        "y_pred_tt = voting_model_tt.predict(X_test)\n",
        "rmse_tt = np.sqrt(mean_squared_error(y_test, y_pred_tt))\n",
        "print(f\"rmse_tt: {rmse_tt}\")\n",
        ">>> 4.685774587644766"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3d6594c",
      "metadata": {
        "id": "e3d6594c"
      },
      "source": [
        "### Randomforest + xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a21212",
      "metadata": {
        "id": "41a21212"
      },
      "outputs": [],
      "source": [
        "categorical_feature = train.select_dtypes(exclude=\"number\").columns\n",
        "categorical_feature\n",
        "\n",
        "train[categorical_feature] = train[categorical_feature].astype(\"category\")\n",
        "test[categorical_feature] = test[categorical_feature].astype(\"category\")\n",
        "train.info(), test.info()\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "train[categorical_feature] = oe.fit_transform(train[categorical_feature])\n",
        "test[categorical_feature] = oe.transform(test[categorical_feature])\n",
        "\n",
        "X = train.drop(columns=\"Book-Rating\")\n",
        "y = train[\"Book-Rating\"]\n",
        "X.shape, y.shape\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X, y, test_size = 0.1, random_state=42\n",
        ")\n",
        "\n",
        "X_train.shape, X_valid.shape, y_train.shape, y_valid.shape\n",
        "X_test = test\n",
        "X_test.shape\n",
        "\n",
        "# model \n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "xgb = XGBRegressor(n_estimators=100, random_state=42)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "rf_pred = rf.predict(X_test)\n",
        "xgb_pred = xgb.predict(X_test)\n",
        "ensemble_pred = (rf_pred + xgb_pred) / 2\n",
        "rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))\n",
        "print(f\"Ensemble RMSE: {rmse}\")\n",
        ">>> 3.442358268797963\n",
        "\n",
        "# submisson Ïóê ÏòÆÍ≤® Ï†ÅÍ≥† Ï†ÄÏû•\n",
        "ensemble_pred = ensemble_pred[:len(df_submit)]\n",
        "df_submit[\"Book-Rating\"] = abs(ensemble_pred)\n",
        "file_name = f\"{base_path}/ensemble_rf_xgb.csv\"\n",
        "df_submit.to_csv(file_name, index=False)\n",
        "pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d1ed43",
      "metadata": {
        "id": "88d1ed43"
      },
      "source": [
        "### SVD + TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66e01fc6",
      "metadata": {
        "id": "66e01fc6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from surprise import Reader, Dataset, SVD\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "sub = pd.read_csv(\"sample_submission.csv\")\n",
        "train['Age'] = np.where(train['Age'] == 0, 15, np.where(train['Age'] >= 80, 80, train['Age']))\n",
        "test['Age'] = np.where(test['Age'] == 0, 15, np.where(test['Age'] >= 80, 80, test['Age']))\n",
        "train['Year-Of-Publication'] = np.where(train['Year-Of-Publication'] == -1, 1997, train['Year-Of-Publication'])\n",
        "test['Year-Of-Publication'] = np.where(test['Year-Of-Publication'] == -1, 1997, test['Year-Of-Publication'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train/Validation Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
        "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†ê ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_mean_rating = train_data.groupby('User-ID')['Book-Rating'].mean().reset_index()\n",
        "user_mean_rating.columns = ['User-ID', 'User-Mean-Rating']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏù¥ Í≤∞Ï∏°ÏπòÏù∏ Í≤ΩÏö∞, Ï†ÑÏ≤¥ ÌèâÍ∑† ÌèâÏ†êÏúºÎ°ú ÎåÄÏ≤¥\n",
        "mean_rating = train_data['Book-Rating'].mean()\n",
        "train_data['User-Mean-Rating'] = train_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "val_data['User-Mean-Rating'] = val_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàò ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_rating_count = train_data.groupby('User-ID')['Book-Rating'].count().reset_index()\n",
        "user_rating_count.columns = ['User-ID', 'User-Rating-Count']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàòÎ•º train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÍ≥º Ï±ÖÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ Ìè¨Ìï®Ìïú ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±\n",
        "user_item_rating_mean = train_data[[\"User-ID\", \"Book-ID\", 'User-Mean-Rating']]\n",
        "# Îç∞Ïù¥ÌÑ∞Î•º Surprise ÎùºÏù¥Î∏åÎü¨Î¶¨ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
        "reader = Reader(rating_scale=(1, 10))\n",
        "data_mean = Dataset.load_from_df(user_item_rating_mean, reader)\n",
        "# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î•º trainsetÏúºÎ°ú Î≥ÄÌôò\n",
        "trainset = data_mean.build_full_trainset()\n",
        "\n",
        "# SVD ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ Íµ¨Ï∂ï Î∞è ÌïôÏäµ\n",
        "svd = SVD(random_state=42,\n",
        "          n_epochs= 100, \n",
        "          lr_all= 0.01,\n",
        "          n_factors = 200,\n",
        "          reg_all = 0.06,\n",
        "          verbose=True)\n",
        "svd.fit(trainset)\n",
        "\n",
        "from surprise.model_selection import cross_validate\n",
        "# ÍµêÏ∞® Í≤ÄÏ¶ùÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ RMSE Í≥ÑÏÇ∞\n",
        "cv_results = cross_validate(svd, data_mean, measures=['RMSE'], cv=5, verbose=True)\n",
        "\n",
        "# ÌèâÍ∑† RMSE Ï∂úÎ†•\n",
        "mean_rmse = np.mean(cv_results['test_rmse'])\n",
        "print(\"Mean RMSE: {:.4f}\".format(mean_rmse))\n",
        "\n",
        "user_factors = svd.pu\n",
        "item_factors = svd.qi\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "user_input = Input(shape=(user_factors.shape[1],), name='User-Factors')\n",
        "item_input = Input(shape=(item_factors.shape[1],), name='Item-Factors')\n",
        "\n",
        "x = Concatenate()([user_input, item_input])\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dense(100, activation='relu')(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "output = Dense(1, activation='linear', name='Rating')(x)\n",
        "\n",
        "model = Model(inputs=[user_input, item_input], outputs=output)\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "user_ids = train_data['User-ID'].apply(lambda x: trainset.to_inner_uid(x))\n",
        "item_ids = train_data['Book-ID'].apply(lambda x: trainset.to_inner_iid(x))\n",
        "\n",
        "train_user_factors = user_factors[user_ids]\n",
        "train_item_factors = item_factors[item_ids]\n",
        "\n",
        "# EarlyStopping ÏΩúÎ∞± Ï†ïÏùò\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
        "\n",
        "history = model.fit([train_user_factors, train_item_factors],\n",
        "                    train_data['Book-Rating'],\n",
        "                    epochs=1000,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[early_stopping])  # EarlyStopping\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Test dataÏùò User-IDÏôÄ Book-IDÎ•º ÎÇ¥Î∂Ä IDÎ°ú Î≥ÄÌôò\n",
        "test_user_ids = val_data['User-ID'].apply(lambda x: trainset.to_inner_uid(x) if x in trainset._raw2inner_id_users else -1)\n",
        "test_item_ids = val_data['Book-ID'].apply(lambda x: trainset.to_inner_iid(x) if x in trainset._raw2inner_id_items else -1)\n",
        "\n",
        "# ÌèâÍ∑† ÏÇ¨Ïö©Ïûê ÏöîÏù∏ Î∞è ÏïÑÏù¥ÌÖú ÏöîÏù∏ Í≥ÑÏÇ∞\n",
        "mean_user_factors = np.mean(user_factors, axis=0)\n",
        "mean_item_factors = np.mean(item_factors, axis=0)\n",
        "\n",
        "# Test dataÏùò ÏÇ¨Ïö©Ïûê ÏöîÏù∏ Î∞è ÏïÑÏù¥ÌÖú ÏöîÏù∏ Í∞ÄÏ†∏Ïò§Í∏∞\n",
        "test_user_factors = np.array([user_factors[i] if i != -1 else mean_user_factors for i in test_user_ids])\n",
        "test_item_factors = np.array([item_factors[i] if i != -1 else mean_item_factors for i in test_item_ids])\n",
        "\n",
        "# TensorFlow Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏòàÏ∏°\n",
        "predictions = model.predict([test_user_factors, test_item_factors]).flatten()\n",
        "\n",
        "# Ïã§Ï†ú ÌèâÏ†ê Î∞è ÏòàÏ∏° ÌèâÏ†ê Í∞ÑÏùò RMSE Í≥ÑÏÇ∞\n",
        "rmse = sqrt(mean_squared_error(val_data['Book-Rating'], predictions))\n",
        "print(\"RMSE on test data: {:.4f}\".format(rmse))\n",
        ">>> 3.7377"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c837e9",
      "metadata": {
        "id": "f6c837e9"
      },
      "source": [
        "### SVD+ÏïÑÏù¥ÌÖúÍ∏∞Î∞òÌòëÏóÖÌïÑÌÑ∞ÎßÅ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb96f45",
      "metadata": {
        "id": "edb96f45"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from surprise import Reader, Dataset, SVD\n",
        "from surprise import KNNWithMeans\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise import accuracy\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "sub = pd.read_csv(\"sample_submission.csv\")\n",
        "train['Age'] = np.where(train['Age'] == 0, 15, np.where(train['Age'] >= 80, 80, train['Age']))\n",
        "test['Age'] = np.where(test['Age'] == 0, 15, np.where(test['Age'] >= 80, 80, test['Age']))\n",
        "train['Year-Of-Publication'] = np.where(train['Year-Of-Publication'] == -1, 1997, train['Year-Of-Publication'])\n",
        "test['Year-Of-Publication'] = np.where(test['Year-Of-Publication'] == -1, 1997, test['Year-Of-Publication'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train/Validation Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
        "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†ê ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_mean_rating = train_data.groupby('User-ID')['Book-Rating'].mean().reset_index()\n",
        "user_mean_rating.columns = ['User-ID', 'User-Mean-Rating']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏù¥ Í≤∞Ï∏°ÏπòÏù∏ Í≤ΩÏö∞, Ï†ÑÏ≤¥ ÌèâÍ∑† ÌèâÏ†êÏúºÎ°ú ÎåÄÏ≤¥\n",
        "mean_rating = train_data['Book-Rating'].mean()\n",
        "train_data['User-Mean-Rating'] = train_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "val_data['User-Mean-Rating'] = val_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàò ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_rating_count = train_data.groupby('User-ID')['Book-Rating'].count().reset_index()\n",
        "user_rating_count.columns = ['User-ID', 'User-Rating-Count']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàòÎ•º train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÍ≥º Ï±ÖÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ Ìè¨Ìï®Ìïú ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±\n",
        "user_item_rating_mean = train_data[[\"User-ID\", \"Book-ID\", 'User-Mean-Rating']]\n",
        "# Îç∞Ïù¥ÌÑ∞Î•º Surprise ÎùºÏù¥Î∏åÎü¨Î¶¨ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
        "reader = Reader(rating_scale=(0, 10))\n",
        "data_mean = Dataset.load_from_df(user_item_rating_mean, reader)\n",
        "# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î•º trainsetÏúºÎ°ú Î≥ÄÌôò\n",
        "trainset = data_mean.build_full_trainset()\n",
        "\n",
        "# SVD ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ Íµ¨Ï∂ï Î∞è ÌïôÏäµ\n",
        "svd = SVD(random_state=42,\n",
        "          n_epochs= 100, \n",
        "          lr_all= 0.01,\n",
        "          n_factors = 200,\n",
        "          reg_all = 0.06,\n",
        "          verbose=True)\n",
        "svd.fit(trainset)\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ ÏÉòÌîåÎßÅ\n",
        "sampled_train_data = train_data.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÍ≥º Ï±ÖÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ Ìè¨Ìï®Ìïú ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±\n",
        "user_item_rating_mean_sampled = sampled_train_data[[\"User-ID\", \"Book-ID\", 'User-Mean-Rating']]\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞Î•º Surprise ÎùºÏù¥Î∏åÎü¨Î¶¨ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
        "reader = Reader(rating_scale=(0, 10))\n",
        "data_mean_sampled = Dataset.load_from_df(user_item_rating_mean_sampled, reader)\n",
        "\n",
        "# ÏÉòÌîåÎßÅÎêú Îç∞Ïù¥ÌÑ∞Î•º trainsetÏúºÎ°ú Î≥ÄÌôò\n",
        "trainset_sampled = data_mean_sampled.build_full_trainset()\n",
        "\n",
        "# ÏïÑÏù¥ÌÖú Í∏∞Î∞ò ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ (KNNWithMeans) Î™®Îç∏ Íµ¨Ï∂ï\n",
        "item_based_cf = KNNWithMeans(k=40, sim_options={'name': 'pearson_baseline', 'user_based': False}, verbose=True)\n",
        "\n",
        "# ÏÉòÌîåÎßÅÎêú Îç∞Ïù¥ÌÑ∞Î°ú Î™®Îç∏ ÌïôÏäµ\n",
        "item_based_cf.fit(trainset_sampled)\n",
        "\n",
        "# SVD ÏòàÏ∏°\n",
        "svd_preds = []\n",
        "for _, row in val_data.iterrows():\n",
        "    svd_preds.append(svd.predict(row['User-ID'], row['Book-ID'])[3])\n",
        "\n",
        "# ÏïÑÏù¥ÌÖú Í∏∞Î∞ò ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ ÏòàÏ∏°\n",
        "item_based_cf_preds = []\n",
        "for _, row in val_data.iterrows():\n",
        "    item_based_cf_preds.append(item_based_cf.predict(row['User-ID'], row['Book-ID'])[3])\n",
        "# ÏïôÏÉÅÎ∏î ÏòàÏ∏° (Í∞ÄÏ§ë ÌèâÍ∑†)\n",
        "ensemble_preds = []\n",
        "alpha = 0.999  # Í∞ÄÏ§ëÏπò ÏÑ§Ï†ï (0Í≥º 1 ÏÇ¨Ïù¥Ïùò Í∞íÏùÑ ÏÑ†ÌÉù)\n",
        "for i in range(len(svd_preds)):\n",
        "    ensemble_preds.append(alpha * svd_preds[i] + (1 - alpha) * item_based_cf_preds[i])\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# SVD RMSE\n",
        "svd_rmse = sqrt(mean_squared_error(val_data['Book-Rating'], svd_preds))\n",
        "print(\"SVD RMSE:\", svd_rmse)\n",
        ">>> 3.3974045875897705\n",
        "\n",
        "# ÏïÑÏù¥ÌÖú Í∏∞Î∞ò ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ RMSE\n",
        "item_based_cf_rmse = sqrt(mean_squared_error(val_data['Book-Rating'], item_based_cf_preds))\n",
        "print(\"Item-Based CF RMSE:\", item_based_cf_rmse)\n",
        ">>> 3.945342860840093\n",
        "\n",
        "# ÏïôÏÉÅÎ∏î RMSE\n",
        "ensemble_rmse = sqrt(mean_squared_error(val_data['Book-Rating'], ensemble_preds))\n",
        "print(\"Ensemble RMSE:\", ensemble_rmse)\n",
        ">>> 3.3974878879011983"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03c63b86",
      "metadata": {
        "id": "03c63b86"
      },
      "source": [
        "### SVD+ÏÇ¨Ïö©ÏûêÍ∏∞Î∞òÌòëÏóÖÌïÑÌÑ∞ÎßÅ ÏïôÏÉÅÎ∏î"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69bfb754",
      "metadata": {
        "id": "69bfb754"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from surprise import Reader, Dataset, SVD\n",
        "from surprise import KNNWithMeans\n",
        "from surprise.model_selection import cross_validate\n",
        "from surprise import accuracy\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "sub = pd.read_csv(\"sample_submission.csv\")\n",
        "train['Age'] = np.where(train['Age'] == 0, 15, np.where(train['Age'] >= 80, 80, train['Age']))\n",
        "test['Age'] = np.where(test['Age'] == 0, 15, np.where(test['Age'] >= 80, 80, test['Age']))\n",
        "train['Year-Of-Publication'] = np.where(train['Year-Of-Publication'] == -1, 1997, train['Year-Of-Publication'])\n",
        "test['Year-Of-Publication'] = np.where(test['Year-Of-Publication'] == -1, 1997, test['Year-Of-Publication'])\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train/Validation Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
        "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†ê ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_mean_rating = train_data.groupby('User-ID')['Book-Rating'].mean().reset_index()\n",
        "user_mean_rating.columns = ['User-ID', 'User-Mean-Rating']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏù¥ Í≤∞Ï∏°ÏπòÏù∏ Í≤ΩÏö∞, Ï†ÑÏ≤¥ ÌèâÍ∑† ÌèâÏ†êÏúºÎ°ú ÎåÄÏ≤¥\n",
        "mean_rating = train_data['Book-Rating'].mean()\n",
        "train_data['User-Mean-Rating'] = train_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "val_data['User-Mean-Rating'] = val_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàò ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_rating_count = train_data.groupby('User-ID')['Book-Rating'].count().reset_index()\n",
        "user_rating_count.columns = ['User-ID', 'User-Rating-Count']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàòÎ•º train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÍ≥º Ï±ÖÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ Ìè¨Ìï®Ìïú ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±\n",
        "user_item_rating_mean = train_data[[\"User-ID\", \"Book-ID\", 'User-Mean-Rating']]\n",
        "# Îç∞Ïù¥ÌÑ∞Î•º Surprise ÎùºÏù¥Î∏åÎü¨Î¶¨ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
        "reader = Reader(rating_scale=(0, 10))\n",
        "data_mean = Dataset.load_from_df(user_item_rating_mean, reader)\n",
        "# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î•º trainsetÏúºÎ°ú Î≥ÄÌôò\n",
        "trainset = data_mean.build_full_trainset()\n",
        "\n",
        "# SVD ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ Íµ¨Ï∂ï Î∞è ÌïôÏäµ\n",
        "svd = SVD(random_state=42,\n",
        "          n_epochs= 100, \n",
        "          lr_all= 0.01,\n",
        "          n_factors = 200,\n",
        "          reg_all = 0.06,\n",
        "          verbose=True)\n",
        "svd.fit(trainset)\n",
        "\n",
        "# ÏÇ¨Ïö©Ïûê Í∏∞Î∞ò ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ (KNNWithMeans) Î™®Îç∏ Íµ¨Ï∂ï\n",
        "user_based_cf = KNNWithMeans(k=40, sim_options={'name': 'pearson_baseline', 'user_based': True}, verbose=True)\n",
        "\n",
        "# ÏÉòÌîåÎßÅÎêú Îç∞Ïù¥ÌÑ∞Î°ú Î™®Îç∏ ÌïôÏäµ\n",
        "user_based_cf.fit(trainset_sampled)\n",
        "\n",
        "# SVD ÏòàÏ∏°\n",
        "svd_preds = []\n",
        "for _, row in val_data.iterrows():\n",
        "    svd_preds.append(svd.predict(row['User-ID'], row['Book-ID']).est)\n",
        "\n",
        "# ÏÇ¨Ïö©Ïûê Í∏∞Î∞ò ÌòëÏóÖ ÌïÑÌÑ∞ÎßÅ ÏòàÏ∏°\n",
        "user_based_cf_preds = []\n",
        "for _, row in val_data.iterrows():\n",
        "    user_based_cf_preds.append(user_based_cf.predict(row['User-ID'], row['Book-ID']).est)\n",
        "# ÏïôÏÉÅÎ∏î ÏòàÏ∏° (Í∞ÄÏ§ë ÌèâÍ∑†)\n",
        "ensemble_preds = []\n",
        "alpha_svd = 0.8\n",
        "alpha_user_based_cf = 0.2\n",
        "for i in range(len(svd_preds)):\n",
        "    ensemble_preds.append(alpha_svd * svd_preds[i] + alpha_user_based_cf * user_based_cf_preds[i])\n",
        "# RMSE Í≥ÑÏÇ∞\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "svd_rmse = sqrt(mean_squared_error(val_data['Book-Rating'], svd_preds))\n",
        "user_based_cf_rmse = sqrt(mean_squared_error(val_data['Book-Rating'], user_based_cf_preds))\n",
        "ensemble_rmse = sqrt(mean_squared_error(val_data['Book-Rating'], ensemble_preds))\n",
        "\n",
        "print(f'SVD RMSE: {svd_rmse}')\n",
        ">>> 3.3974045875897705\n",
        "\n",
        "print(f'User-Based CF RMSE: {user_based_cf_rmse}')\n",
        ">>> 3.6803972720108873\n",
        "\n",
        "print(f'Ensemble RMSE: {ensemble_rmse}')\n",
        ">>> 3.419238697809746"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20c0765f",
      "metadata": {
        "id": "20c0765f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c16060a3",
      "metadata": {
        "id": "c16060a3"
      },
      "source": [
        "# Í∑∏Î¶¨Îìú ÏÑúÏπò, ÍµêÏ∞® Í≤ÄÏ¶ù"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bab2b165",
      "metadata": {
        "id": "bab2b165"
      },
      "source": [
        "\n",
        "üôã‚Äç‚ôÇÔ∏è Í∞ÄÏÑ§ 6: **ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÏµúÏ†ÅÌôî**Î•º ÌÜµÌï¥ **Ï∂îÏ≤ú ÏãúÏä§ÌÖúÏùò ÏÑ±Îä•ÏùÑ Ìñ•ÏÉÅ**ÏãúÌÇ¨ Ïàò ÏûàÎã§.\n",
        "\n",
        "- Í≤∞Í≥ºÎ¨º: Í∑∏Î¶¨Îìú ÏÑúÏπò, ÎûúÎç§ ÏÑúÏπò, Î≤†Ïù¥ÏßÄÏïà ÏµúÏ†ÅÌôî Îì±Ïùò Í∏∞Î≤ïÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏Ïùò ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞Î•º ÏµúÏ†ÅÌôîÌïúÎã§. ÏµúÏ†ÅÌôîÎêú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞Î•º ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïòÍ≥†, Í∏∞Î≥∏ ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞Î•º ÏÇ¨Ïö©Ìïú Î™®Îç∏Í≥º ÏÑ±Îä•ÏùÑ ÎπÑÍµêÌïòÏó¨ ÏµúÏ†ÅÌôî Í∏∞Î≤ïÏùò Ìö®Í≥ºÎ•º Î∂ÑÏÑùÌïúÎã§.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "658f4e74",
      "metadata": {
        "id": "658f4e74"
      },
      "source": [
        "### GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85ecd31e",
      "metadata": {
        "id": "85ecd31e"
      },
      "outputs": [],
      "source": [
        "from surprise.model_selection import GridSearchCV, cross_validate\n",
        "\n",
        "param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n",
        "              'reg_all': [0.4, 0.6]}\n",
        "\n",
        "gridcv = GridSearchCV(algo_class=SVD, \n",
        "                      param_grid=param_grid, \n",
        "                      measures=[\"rmse\"], cv=3)\n",
        "gridcv.fit(data)\n",
        "\n",
        "gridcv.best_estimator\n",
        "gridcv.best_params[\"rmse\"]\n",
        "gridcv.best_score\n",
        "gridcv.algo_class\n",
        "best_svd = gridcv.best_estimator[\"rmse\"]\n",
        "best_svd.fit(X_train)\n",
        "X_test = [(f\"TEST_{uid[6:]}\", iid, rating) for uid, iid, rating in X_test]\n",
        "X_test\n",
        "\n",
        "predictions_bsvd = best_svd.test(X_test)\n",
        "predictions_bsvd\n",
        "\n",
        "predictions_bsvd[0]\n",
        "accuracy.rmse(predictions_bsvd)\n",
        "\n",
        "df_pred_svd = pd.DataFrame(predictions_bsvd)\n",
        "df_pred_svd.head()\n",
        "\n",
        "y_pred_svd = df_pred_svd[\"est\"].unique()\n",
        "y_pred_svd\n",
        "\n",
        "y_pred_svd = [pred.est for pred in predictions_bsvd]\n",
        "y_pred_svd = np.array(y_pred_svd)\n",
        "y_pred_svd.shape\n",
        "\n",
        "df_submit = pd.read_csv(f\"{base_path}/sample_submission.csv\")\n",
        "df_submit.head()\n",
        "\n",
        "y_pred_svd = y_pred_svd[:len(df_submit)]\n",
        "y_pred_svd\n",
        "\n",
        "df_submit[\"Book-Rating\"] = abs(y_pred_svd)\n",
        "\n",
        "file_name = f\"{base_path}/submit_bsvd.csv\"\n",
        "df_submit.to_csv(file_name, index=False)\n",
        "pd.read_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8b765ff",
      "metadata": {
        "id": "c8b765ff"
      },
      "outputs": [],
      "source": [
        "from surprise.model_selection import GridSearchCV, cross_validate\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ \n",
        "# Ïú†Ï†Ä - ÏïÑÏù¥ÌÖú - ÌèâÏ†ê Îç∞Ïù¥ÌÑ∞ÏÖã ÎßåÎì§Í∏∞ \n",
        "df = train_data[[\"User-ID\", \"Book-Title\", \"Book-Rating\"]]\n",
        "\n",
        "# reader \n",
        "r_min = df['Book-Rating'].min()\n",
        "r_max = df['Book-Rating'].max()\n",
        "reader = Reader(rating_scale=(r_min, r_max))\n",
        "\n",
        "\n",
        "# Dataset.load_from_df Î°ú Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌï©ÎãàÎã§.\n",
        "data = Dataset.load_from_df(\n",
        "    df[[\"User-ID\", \"Book-Title\", \"Book-Rating\"]], \n",
        "    reader)\n",
        "\n",
        "# GridSearchCVÎ°ú ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞ ÌÉêÏÉâ\n",
        "param_grid = {'n_epochs': [5, 10, 20], \n",
        "              'lr_all': [0.002, 0.005],\n",
        "              'n_factors' : [10, 20],\n",
        "              'reg_all': [0.4, 0.6]}\n",
        "\n",
        "gridcv = GridSearchCV(algo_class=SVD, \n",
        "                      param_grid=param_grid, \n",
        "                      measures=[\"rmse\"], cv=3)\n",
        "\n",
        "gridcv.fit(data)\n",
        "gridcv.best_estimator\n",
        "gridcv.best_params[\"rmse\"]\n",
        "gridcv.best_score\n",
        "\n",
        "# Í∞ÄÏû• ÏûòÎÇòÏò® ÌååÎùºÎØ∏ÌÑ∞Î°ú ÌõàÎ†® Î∞è ÌèâÍ∞Ä \n",
        "pd.DataFrame(gridcv.cv_results)\n",
        "best_svd = gridcv.best_estimator[\"rmse\"]\n",
        "best_svd.fit(X_train)\n",
        "predictions_bsvd = best_svd.test(X_test)\n",
        "accuracy.rmse(predictions_bsvd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e09c248",
      "metadata": {
        "id": "5e09c248"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from surprise.model_selection import GridSearchCV\n",
        "\n",
        "# Train/Validation Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
        "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†ê ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_mean_rating = train_data.groupby('User-ID')['Book-Rating'].mean().reset_index()\n",
        "user_mean_rating.columns = ['User-ID', 'User-Mean-Rating']\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏù¥ Í≤∞Ï∏°ÏπòÏù∏ Í≤ΩÏö∞, Ï†ÑÏ≤¥ ÌèâÍ∑† ÌèâÏ†êÏúºÎ°ú ÎåÄÏ≤¥\n",
        "mean_rating = train_data['Book-Rating'].mean()\n",
        "train_data['User-Mean-Rating'] = train_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "val_data['User-Mean-Rating'] = val_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàò ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_rating_count = train_data.groupby('User-ID')['Book-Rating'].count().reset_index()\n",
        "user_rating_count.columns = ['User-ID', 'User-Rating-Count']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàòÎ•º train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "\n",
        "user_item_rating_mean = train_data[[\"User-ID\", \"Book-ID\", 'User-Mean-Rating']]\n",
        "reader = Reader(rating_scale=(0, 10))\n",
        "data_mean = Dataset.load_from_df(user_item_rating_mean, reader)\n",
        "\n",
        "# GridSearchCVÎ°ú ÏµúÏ†Å ÌååÎùºÎØ∏ÌÑ∞ ÌÉêÏÉâ\n",
        "param_grid = {'n_epochs': [5, 20, 30], \n",
        "              'lr_all': [0.002, 0.005, 0.007],\n",
        "              'n_factors' : [20, 50, 100],\n",
        "              'reg_all': [0.02, 0.04]}\n",
        "\n",
        "gridcv = GridSearchCV(algo_class=SVD, \n",
        "                      param_grid=param_grid, \n",
        "                      measures=[\"rmse\"], cv=3)\n",
        "\n",
        "gridcv.fit(data_mean)\n",
        "gridcv.best_estimator\n",
        "gridcv.best_params[\"rmse\"]\n",
        "gridcv.best_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2996f917",
      "metadata": {
        "id": "2996f917"
      },
      "source": [
        "### Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0614a738",
      "metadata": {
        "id": "0614a738"
      },
      "outputs": [],
      "source": [
        "cv_result = cross_validate(best_svd, data, measures=[\"rmse\"], n_jobs=2, verbose=True)\n",
        "\n",
        "pd.DataFrame(cv_result)\n",
        "\n",
        "def get_top_n(predictions, n=10):\n",
        "    # Í∞Å ÏÇ¨Ïö©ÏûêÏùò ÏòàÏ∏°Îç∞Ïù¥ÌÑ∞Î•º defaultdictÏóê Ï†ÄÏû•\n",
        "    top_n = defaultdict(list)\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_n[uid].append((iid, est))\n",
        "\n",
        "    # Ï†ïÎ†¨ ÌõÑ Top N Í∞úÎßå Ï†ÄÏû•\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[uid] = user_ratings[:n]\n",
        "\n",
        "    return top_n\n",
        "\n",
        "top_num = get_top_n(predictions_bsvd, 10)\n",
        "\n",
        "df_cust_recomm = pd.DataFrame(top_num.items())\n",
        "df_cust_recomm.shape\n",
        "\n",
        "stock_desc = df_valid[\n",
        "    [\"ID\", \"Book-ID\",\"Book-Rating\"]].drop_duplicates(\"ID\").set_index(\"Book-ID\")\n",
        "stock_desc\n",
        "stock_desc.loc[\"TRAIN_000004\"]\n",
        "\n",
        "df_recomm = pd.DataFrame(df_cust_recomm.iloc[:, 1]).set_index('Book-ID').join(stock_desc)\n",
        "df_recomm\n",
        "\n",
        "y_pred_recomm = df_recomm[\"Book-Rating\"].unique()\n",
        "y_pred_recomm\n",
        "\n",
        "df_submit = pd.read_csv(f\"{base_path}/sample_submission.csv\")\n",
        "df_submit\n",
        "\n",
        "df_submit[\"Book-Rating\"] = abs(y_pred_recomm)\n",
        "file_name = f\"{base_path}/submit.csv\"\n",
        "from google.colab import files\n",
        "\n",
        "files.download(file_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1675c44",
      "metadata": {
        "id": "f1675c44"
      },
      "outputs": [],
      "source": [
        "# cross validation \n",
        "# Regularization Ï†ÅÏö© \n",
        "\n",
        "from surprise import SVD\n",
        "from surprise.model_selection import cross_validate\n",
        "\n",
        "# Ïú†Ï†Ä - ÏïÑÏù¥ÌÖú - ÌèâÏ†ê Îç∞Ïù¥ÌÑ∞ÏÖã ÎßåÎì§Í∏∞ \n",
        "df = train[['User-ID','Book-Title', 'Book-Rating']]\n",
        "\n",
        "# reader \n",
        "r_min = df['Book-Rating'].min()\n",
        "r_max = df['Book-Rating'].max()\n",
        "reader = Reader(rating_scale=(r_min, r_max))\n",
        "\n",
        "\n",
        "# Dataset.load_from_df Î°ú Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌï©ÎãàÎã§.\n",
        "data = Dataset.load_from_df(\n",
        "    df[['User-ID','Book-Title', 'Book-Rating']], \n",
        "    reader)\n",
        "\n",
        "svd_1 = SVD(n_factors=50, n_epochs=10, lr_all=0.005, reg_all=0.4, random_state=42)\n",
        "svd_2 = SVD(n_factors=50, n_epochs=15, lr_all=0.01, reg_all=0.2, random_state=42)\n",
        "svd_3 = SVD(n_factors=50, n_epochs=20, lr_all=0.02, reg_all=0.1, random_state=42)\n",
        "\n",
        "\n",
        "# results_1 = cross_validate(svd_1, data, measures=['RMSE'], cv=5, n_jobs=2, verbose=False)\n",
        "results_2 = cross_validate(svd_2, data, measures=['RMSE'], cv=5, n_jobs=2, verbose=False)\n",
        "# results_3 = cross_validate(svd_3, data, measures=['RMSE'], cv=5, n_jobs=2, verbose=False)\n",
        "\n",
        "# print(\"SVD 1 RMSE:\", results_1['test_rmse'].mean())\n",
        "print(\"SVD 2 RMSE:\", results_2['test_rmse'].mean())\n",
        "# print(\"SVD 3 RMSE:\", results_3['test_rmse'].mean())\n",
        "\n",
        "# # Best model\n",
        "best_svd = svd_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2be10190",
      "metadata": {
        "id": "2be10190"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7c84ec01",
      "metadata": {
        "id": "7c84ec01"
      },
      "source": [
        "# Îî•Îü¨Îãù"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08039747",
      "metadata": {
        "id": "08039747"
      },
      "source": [
        "üôã‚Äç‚ôÇÔ∏è Í∞ÄÏÑ§ 4: **Îî• Îü¨Îãù Í∏∞Î∞òÏùò Ï∂îÏ≤ú ÏãúÏä§ÌÖú**Ïù¥ ÎèÑÏÑú ÌèâÏ†ê ÏòàÏ∏°Ïóê Ìö®Í≥ºÏ†ÅÏùº Í≤ÉÏù¥Îã§.\n",
        "\n",
        "- Í≤∞Í≥ºÎ¨º: **Îî• Îü¨Îãù Î™®Îç∏(Ïã†Í≤ΩÎßù Í∏∞Î∞ò Ï∂îÏ≤ú Î™®Îç∏ ÎòêÎäî Ïû†Ïû¨ ÏöîÏù∏ Î™®Îç∏)**ÏùÑ Íµ¨ÌòÑÌïòÍ≥†, Î™®Îç∏Ïùò **ÌèâÏ†ê ÏòàÏ∏° ÏÑ±Îä•**ÏùÑ ÌèâÍ∞ÄÌïúÎã§. Îã§Î•∏ Ï∂îÏ≤ú ÏãúÏä§ÌÖúÍ≥º ÏÑ±Îä•ÏùÑ ÎπÑÍµêÌïòÏó¨ Îî• Îü¨Îãù Í∏∞Î∞ò Ï∂îÏ≤ú ÏãúÏä§ÌÖúÏùò Ìö®Í≥ºÎ•º Î∂ÑÏÑùÌïúÎã§."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99322aae",
      "metadata": {
        "id": "99322aae"
      },
      "outputs": [],
      "source": [
        "! pip install pad_sequences\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Embedding, Conv1D, MaxPooling1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = train_data[[\"User-ID\", \"Book-Title\", \"Book-Rating\"]]\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(num_words=5000) # num_words ; Îã®Ïñ¥Ïùò ÎπàÎèÑÏàòÍ∞Ä ÎÜíÏùÄ ÏàúÏúºÎ°ú nÍ∞ú\n",
        "tokenizer.fit_on_texts(df['Book-Title'])\n",
        "X = tokenizer.texts_to_sequences(df['Book-Title'])\n",
        "X = pad_sequences(X, maxlen=100)\n",
        "\n",
        "# Îç∞Ïù¥ÌÑ∞ ÏÖã Î∂ÑÎ¶¨ \n",
        "y = df['Book-Rating']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# cnn Î™®Îç∏ Ï†ïÏùò \n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=32, input_length=100))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=10, activation='relu'))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=128, epochs=50, verbose=1, validation_split=0.2)\n",
        "\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "142a5e4e",
      "metadata": {
        "id": "142a5e4e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.regularizers import l1_l2\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='mean_squared_error')\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=500, verbose=1, validation_split=0.2)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        ">>> 3.8194436625733723"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a06655",
      "metadata": {
        "id": "70a06655"
      },
      "source": [
        "### Ï†ïÍ∑úÌôî, Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dbc6e23",
      "metadata": {
        "id": "2dbc6e23"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=500, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        ">>> 3.848602675184224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "153db35d",
      "metadata": {
        "id": "153db35d"
      },
      "outputs": [],
      "source": [
        "### MinMaxScalerÎ•º Ïù¥Ïö©Ìïú ÏΩòÌÖêÏ∏†Í∏∞Î∞òÎî•Îü¨Îãù"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3a895d",
      "metadata": {
        "id": "4c3a895d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "train = pd.read_csv(\"reco/train.csv\")\n",
        "test = pd.read_csv(\"reco/test.csv\")\n",
        "sub = pd.read_csv(\"reco/sample_submission.csv\")\n",
        "train['Age'] = np.where((train['Age'] == 0) | (train['Age'] >= 80), 35, train['Age'])\n",
        "test['Age'] = np.where((test['Age'] == 0) | (test['Age'] >= 80), 35, test['Age'])\n",
        "train['Year-Of-Publication'] = np.where(train['Year-Of-Publication'] == -1, 1997, train['Year-Of-Publication'])\n",
        "test['Year-Of-Publication'] = np.where(test['Year-Of-Publication'] == -1, 1997, test['Year-Of-Publication'])\n",
        "# Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Î≤îÏ£ºÌòï Ïó¥ÏùÑ one-hot Ïù∏ÏΩîÎî© ÏãúÏºúÏ§ÄÎã§. \n",
        "# 'User-ID', 'Book-ID', 'Location', 'Book-Title', 'Book-Author', 'Publisher' Îã§ÌñàÎçîÎãà Ïª§ÎÑêÏù¥ Ï£ΩÎäîÎã§. Ï±óÏßÄÌîºÌã∞ ÏßàÎ¨∏\n",
        "# 1. Book-AuthorÏôÄ PublisherÎßå ÏõêÌï´Ïù∏ÏΩîÎî©\n",
        "train_encoded = pd.get_dummies(train, columns=['Book-Author', 'Publisher'])\n",
        "# 2. Î≤îÏ£ºÌòï Î≥ÄÏàòÏóêÏÑú Í∞ÄÏû• ÎπàÎèÑÍ∞Ä ÎÜíÏùÄ ÏÉÅÏúÑ NÍ∞úÏùò Î≤îÏ£ºÎßå ÏÇ¨Ïö©ÌïòÏó¨ Ïõê-Ìï´ Ïù∏ÏΩîÎî©\n",
        "# ÏÉÅÏúÑ NÍ∞ú Î≤îÏ£ºÎßå Ïú†ÏßÄ\n",
        "N = 200\n",
        "top_N_authors = train['Book-Author'].value_counts().head(N).index\n",
        "top_N_publishers = train['Publisher'].value_counts().head(N).index\n",
        "\n",
        "# ÏÉÅÏúÑ NÍ∞ú Î≤îÏ£ºÏóê ÏÜçÌïòÏßÄ ÏïäÎäî Í≤ΩÏö∞ 'Other'Î°ú ÎåÄÏ≤¥\n",
        "train['Book-Author'] = train['Book-Author'].apply(lambda x: x if x in top_N_authors else 'Other')\n",
        "train['Publisher'] = train['Publisher'].apply(lambda x: x if x in top_N_publishers else 'Other')\n",
        "\n",
        "# Ïõê-Ìï´ Ïù∏ÏΩîÎî© Ï†ÅÏö©\n",
        "train_encoded = pd.get_dummies(train, columns=['Book-Author', 'Publisher'])\n",
        "scaler = MinMaxScaler()\n",
        "train_encoded[['Age', 'Year-Of-Publication']] = scaler.fit_transform(train_encoded[['Age', 'Year-Of-Publication']])\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = train_encoded.drop('Book-Rating', axis=1)\n",
        "y = train_encoded['Book-Rating']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=1000, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
        "y_pred = model.predict(X_test)\n",
        "mse = tf.keras.losses.mean_squared_error(y_test, y_pred)\n",
        "rmse = tf.sqrt(mse)\n",
        "print(\"Root Mean Squared Error:\", rmse.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5df0ffd0",
      "metadata": {
        "id": "5df0ffd0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a6468dcc",
      "metadata": {
        "id": "a6468dcc"
      },
      "source": [
        "# LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e2cafb",
      "metadata": {
        "id": "38e2cafb"
      },
      "outputs": [],
      "source": [
        "# object typeÏùÑ catetoryÌôî ÏãúÌÇ§Í∏∞\n",
        "\n",
        "for column in train.columns:\n",
        "    if train[column].dtype == 'object':\n",
        "        train[column] = train[column].astype('category')\n",
        "# Îç∞Ïù¥ÌÑ∞Î•º ÌïôÏäµ Î∞è ÌÖåÏä§Ìä∏ ÏÑ∏Ìä∏Î°ú Î∂ÑÌï†\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, train['Book-Rating'], test_size=0.2, random_state=42)\n",
        "\n",
        "# ÌöåÍ∑Ä Î™®Îç∏ ÌïôÏäµ\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ÏòàÏ∏° Î∞è ÏÑ±Îä• ÌèâÍ∞Ä\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        ">>> 3.813516138856568"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVD++"
      ],
      "metadata": {
        "id": "YH7iB64oQqu5"
      },
      "id": "YH7iB64oQqu5"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from surprise import Reader, Dataset, SVD, SVDpp\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "sub = pd.read_csv(\"sample_submission.csv\")\n",
        "train['Age'] = np.where(train['Age'] == 0, 15, np.where(train['Age'] >= 80, 80, train['Age']))\n",
        "test['Age'] = np.where(test['Age'] == 0, 15, np.where(test['Age'] >= 80, 80, test['Age']))\n",
        "train['Year-Of-Publication'] = np.where(train['Year-Of-Publication'] == -1, 1997, train['Year-Of-Publication'])\n",
        "test['Year-Of-Publication'] = np.where(test['Year-Of-Publication'] == -1, 1997, test['Year-Of-Publication'])\n",
        "# train = train[~train['User-ID'].isin(single_rating_users.index)] svd++ÏóêÏÑú ÏãúÎèÑÌïòÎäîÍ≤É\n",
        "# test = test[~test['User-ID'].isin(single_rating_users.index)]\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Train/Validation Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†\n",
        "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†ê ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_mean_rating = train_data.groupby('User-ID')['Book-Rating'].mean().reset_index()\n",
        "user_mean_rating.columns = ['User-ID', 'User-Mean-Rating']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_mean_rating, on='User-ID', how='left')\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏù¥ Í≤∞Ï∏°ÏπòÏù∏ Í≤ΩÏö∞, Ï†ÑÏ≤¥ ÌèâÍ∑† ÌèâÏ†êÏúºÎ°ú ÎåÄÏ≤¥\n",
        "mean_rating = train_data['Book-Rating'].mean()\n",
        "train_data['User-Mean-Rating'] = train_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "val_data['User-Mean-Rating'] = val_data['User-Mean-Rating'].fillna(mean_rating)\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàò ÌäπÏÑ± ÏÉùÏÑ±\n",
        "user_rating_count = train_data.groupby('User-ID')['Book-Rating'].count().reset_index()\n",
        "user_rating_count.columns = ['User-ID', 'User-Rating-Count']\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∞Ä ÌöüÏàòÎ•º train_dataÏôÄ val_dataÏóê Î≥ëÌï©\n",
        "train_data = train_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "val_data = val_data.merge(user_rating_count, on='User-ID', how='left')\n",
        "\n",
        "# ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÍ≥º Ï±ÖÎ≥Ñ ÌèâÍ∑† ÌèâÏ†êÏùÑ Ìè¨Ìï®Ìïú ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±\n",
        "user_item_rating_mean = train_data[[\"User-ID\", \"Book-ID\", 'User-Mean-Rating']]\n",
        "# Îç∞Ïù¥ÌÑ∞Î•º Surprise ÎùºÏù¥Î∏åÎü¨Î¶¨ ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
        "reader = Reader(rating_scale=(0, 10))\n",
        "data_mean = Dataset.load_from_df(user_item_rating_mean, reader)\n",
        "# Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞Î•º trainsetÏúºÎ°ú Î≥ÄÌôò\n",
        "trainset = data_mean.build_full_trainset()\n",
        "\n",
        "# SVD ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ Íµ¨Ï∂ï Î∞è ÌïôÏäµ\n",
        "svdpp = SVDpp(random_state=42,\n",
        "          n_epochs= 50, \n",
        "          verbose=True)\n",
        "svdpp.fit(trainset) #svdppÎ•º svdÎ°ú Î∞îÍæ∏Í∏∞Îßå ÌïòÎ©¥ Îêå\n",
        "# Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú ÏòàÏ∏°\n",
        "val_data[\"predicted_rating\"] = val_data.apply(lambda row: svdpp.predict(row[\"User-ID\"], row[\"Book-ID\"]).est, axis=1)\n",
        "# ÏÑ±Îä• ÌèâÍ∞Ä (RMSE)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "rmse = mean_squared_error(val_data[\"Book-Rating\"], val_data[\"predicted_rating\"], squared=False)\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "# ÏõêÎûòÏùò ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Book-Rating ÏòàÏ∏°\n",
        "test[\"Book-Rating\"] = test.apply(lambda row: svdpp.predict(row[\"User-ID\"], row[\"Book-ID\"]).est, axis=1)\n",
        "\n",
        "# Í≤∞Í≥ºÎ•º Ï†ÄÏû•Ìï† Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ± Î∞è Í≤∞Ìï©\n",
        "sub = sub.copy()\n",
        "sub[\"Book-Rating\"] = test[\"Book-Rating\"]\n",
        "\n",
        "# Í≤∞Í≥ºÎ•º CSV ÌååÏùºÎ°ú Ï†ÄÏû•\n",
        "sub.to_csv(f\"svd{rmse:.4f}.csv\", index=False)\n",
        "```"
      ],
      "metadata": {
        "id": "RigjZzDoQpLj"
      },
      "id": "RigjZzDoQpLj",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8 (NGC 22.09 / TensorFlow 2.9.1) on Backend.AI",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
